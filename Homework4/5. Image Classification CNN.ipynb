{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59b577aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.colors\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "from tqdm import tqdm_notebook \n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import cv2\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.datasets import make_blobs\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm \n",
    "import tarfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa0a836b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\shame\\\\Downloads\\\\MLQ5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8d518",
   "metadata": {},
   "source": [
    "# a) Download Data (unpickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180f46ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch_1\n",
      "train_batch_2\n",
      "train_batch_3\n",
      "train_batch_4\n",
      "validation_batch\n",
      "test_batch\n"
     ]
    }
   ],
   "source": [
    "directory = 'C:\\\\Users\\\\shame\\\\Downloads\\\\MLQ5\\\\cifar-10-batches-py'\n",
    "\n",
    "# Extracting image files to dictionary using the unpickling code given in cifar readme page:\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "train_and_test_dicts = []\n",
    "\n",
    "# f_path = os.path.join(directory, file)\n",
    "# # train_and_test_dicts.append(unpickle(f_path))\n",
    "# tt = unpickle(r\"C:\\Users\\shame\\Downloads\\MLQ5\\cifar-10-batches-py\\data_batch_1\")\n",
    "# print(tt)\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    f_path = os.path.join(directory, file)\n",
    "#     print(f_path)\n",
    "    train_and_test_dicts.append(unpickle(f_path))\n",
    "    \n",
    "for i in range(1, len(train_and_test_dicts) + 1):\n",
    "    # Creating training batches, validation batch and final test batch dictionaries:\n",
    "    if i == 6:\n",
    "        batch_name = \"test_batch\"\n",
    "        print(batch_name)\n",
    "        globals()[batch_name] = train_and_test_dicts[i-1]\n",
    "    elif i == 5:\n",
    "        batch_name = \"validation_batch\"\n",
    "        print(batch_name)\n",
    "        globals()[batch_name] = train_and_test_dicts[i-1]\n",
    "    else:\n",
    "        batch_name = \"train_batch_\" + str(i)\n",
    "        print(batch_name)\n",
    "        globals()[batch_name] = train_and_test_dicts[i-1]\n",
    "        \n",
    "# Get the metadata file:\n",
    "meta_file = r'C:\\Users\\shame\\Downloads\\MLQ5\\batches.meta'\n",
    "# assert os.path.isfile(meta_file)\n",
    "meta_data = unpickle(meta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e21052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  (10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data: \", train_batch_1[b'data'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8a28c9",
   "metadata": {},
   "source": [
    "# a) Normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3cfbfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data Min: -1.000u, Max: 1.000 \n",
      "\n",
      "Normalized data Min: -1.000u, Max: 1.000 \n",
      "\n",
      "Normalized data Min: -1.000u, Max: 1.000 \n",
      "\n",
      "Normalized data Min: -1.000u, Max: 1.000 \n",
      "\n",
      "Normalized data Min: -1.000u, Max: 1.000 \n",
      "\n",
      "Normalized data Min: -1.000u, Max: 1.000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize the images so that the maximum and minimum pixel values are between -1 and +1.\n",
    "# Referring to https://machinelearningmastery.com/how-to-manually-scale-image-pixel-data-for-deep-learning/\n",
    "\n",
    "# To approach each set:\n",
    "data = [train_batch_1, train_batch_2, train_batch_3, train_batch_4, validation_batch, test_batch]\n",
    "\n",
    "for batch in data:\n",
    "    batch[b'data'] = batch[b'data'].astype('float32')\n",
    "    # NORMALIZE\n",
    "    batch[b'data'] = (2.0*batch[b'data'] / 255.0) - 1\n",
    "    print('Normalized data Min: %.3fu, Max: %.3f' % (batch[b'data'].min(), batch[b'data'].max()),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3955580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(label,data):\n",
    "\n",
    "    data_set_test = []\n",
    "    for y,x in zip(label,data):\n",
    "        # Reshaping the 3072 pixels to nparrays of R, G, B pixels pf size 1024 each. \n",
    "        X_r = np.reshape(x[:1024],(32,32))\n",
    "        X_g = np.reshape(x[1024:2048],(32,32))\n",
    "        X_b = np.reshape(x[2048:],(32,32)) \n",
    "        # Creating 3 channels to be given as an input to the first layer of Neural Network.\n",
    "        X = np.stack((X_r,X_g,X_b),0) \n",
    "        data_set_test.append([X,y])\n",
    "    return data_set_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ea5e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count in training set : 40000\n",
      "Image count in validation set : 10000\n",
      "Image count in test set : 10000\n"
     ]
    }
   ],
   "source": [
    "#GENERATING TRAINING DATASET\n",
    "appended_training_set = []\n",
    "i = 0\n",
    "for batch in data:\n",
    "    if i<4:\n",
    "        appended_training_set.append(get_dataset(batch[b'labels'], batch[b'data']))\n",
    "        i += 1\n",
    "\n",
    "training_set =[]\n",
    "for i in range(len(appended_training_set)):\n",
    "    for j in range(len(appended_training_set[i])):\n",
    "        training_set.append(appended_training_set[i][j])       \n",
    "print(\"Image count in training set :\",len(training_set))\n",
    "np.save(\"training_data.npy\",training_set) #saving it\n",
    "\n",
    "#GENERATING VALIDATION DATASET\n",
    "validation_set = get_dataset(validation_batch[b'labels'], validation_batch[b'data'])\n",
    "print(\"Image count in validation set :\",len(validation_set))\n",
    "np.save(\"validation_data.npy\",validation_set)\n",
    "\n",
    "\n",
    "#GENERATING TEST DATASET\n",
    "test_set = get_dataset(test_batch[b'labels'], test_batch[b'data'])\n",
    "print(\"Image count in test set :\",len(test_set))\n",
    "np.save(\"test_data.npy\",test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7015c00",
   "metadata": {},
   "source": [
    "# b) Implement CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fffb83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZING \n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.02\n",
    "num_epochs = 20\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61cd7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING DATALOADERS for model\n",
    "\n",
    "# Referred to: https://pytorch.org/docs/stable/data.html\n",
    "num_workers = 5\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(training_set)\n",
    "train_idx = list(range(num_train))\n",
    "np.random.shuffle(train_idx)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_valid = len(validation_set)\n",
    "valid_idx = list(range(num_valid))\n",
    "np.random.shuffle(valid_idx)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_test = len(test_set)\n",
    "test_idx = list(range(num_test))\n",
    "np.random.shuffle(test_idx)\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f08e116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN MODEL\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.convolutional_layer_1 = nn.Conv2d(in_channels = 3, out_channels = 6, kernel_size = 5)\n",
    "\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2)\n",
    "        \n",
    "        self.convolutional_layer_2 = nn.Conv2d(in_channels = 6, out_channels = 16, kernel_size = 5)\n",
    "        \n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2)\n",
    "        \n",
    "        self.feedforward_1 = nn.Linear(5*5*16,120)\n",
    "        \n",
    "        self.feedforward_2 = nn.Linear(120,84)\n",
    "        \n",
    "        self.feedforward_3 = nn.Linear(84,num_classes)\n",
    "        \n",
    "        self.softmax_final = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.convolutional_layer_1(x)\n",
    "        \n",
    "        out = self.max_pool1(F.relu(out))\n",
    "        \n",
    "        out = self.convolutional_layer_2(out)\n",
    "        \n",
    "        out = self.max_pool2(F.relu(out))\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        out = out.view(-1, 5*5*16)\n",
    "        \n",
    "        out = self.feedforward_1(out)\n",
    "        \n",
    "        out = self.feedforward_2(F.relu(out))\n",
    "        \n",
    "        out = self.feedforward_3(F.relu(out))\n",
    "        \n",
    "        # out = self.softmax_final(out)  # Remove softmax when using CrossEntropyLoss\n",
    "        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d988861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER TUNING\n",
    "\n",
    "#calling model\n",
    "model = CNN(num_classes)\n",
    "if torch.cuda.is_available():\n",
    "    model.to(device)\n",
    "\n",
    "## Set loss function:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "## Set optimizer:\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "\n",
    "total_step = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f50a2a",
   "metadata": {},
   "source": [
    "# c) TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ba5e6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.779714 \tValidation Loss: 1.568021\n",
      "SAVING NEW BEST MODEL\n",
      "Epoch: 1 \tTraining Loss: 1.549058 \tValidation Loss: 1.598523\n",
      "Epoch: 2 \tTraining Loss: 1.502536 \tValidation Loss: 1.525787\n",
      "SAVING NEW BEST MODEL\n",
      "Epoch: 3 \tTraining Loss: 1.468298 \tValidation Loss: 1.465157\n",
      "SAVING NEW BEST MODEL\n",
      "Epoch: 4 \tTraining Loss: 1.446341 \tValidation Loss: 1.552579\n",
      "Epoch: 5 \tTraining Loss: 1.439225 \tValidation Loss: 1.478067\n",
      "Epoch: 6 \tTraining Loss: 1.444138 \tValidation Loss: 1.526855\n",
      "Epoch: 7 \tTraining Loss: 1.448623 \tValidation Loss: 1.562189\n",
      "Epoch: 8 \tTraining Loss: 1.433763 \tValidation Loss: 1.541322\n",
      "Epoch: 9 \tTraining Loss: 1.460772 \tValidation Loss: 1.563084\n",
      "Epoch: 10 \tTraining Loss: 1.452895 \tValidation Loss: 1.612883\n",
      "Epoch: 11 \tTraining Loss: 1.473717 \tValidation Loss: 1.636015\n",
      "Epoch: 12 \tTraining Loss: 1.478433 \tValidation Loss: 1.574545\n",
      "Epoch: 13 \tTraining Loss: 1.486904 \tValidation Loss: 1.650097\n",
      "Epoch: 14 \tTraining Loss: 1.507689 \tValidation Loss: 1.607216\n",
      "Epoch: 15 \tTraining Loss: 1.529688 \tValidation Loss: 1.679250\n",
      "Epoch: 16 \tTraining Loss: 1.530464 \tValidation Loss: 1.678725\n",
      "Epoch: 17 \tTraining Loss: 1.540852 \tValidation Loss: 1.703208\n",
      "Epoch: 18 \tTraining Loss: 1.559914 \tValidation Loss: 1.767134\n",
      "Epoch: 19 \tTraining Loss: 1.567764 \tValidation Loss: 1.889893\n"
     ]
    }
   ],
   "source": [
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "list_train_losses = []\n",
    "list_val_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    class_correct_train = list(0. for i in range(10))\n",
    "    class_total_train = list(0. for i in range(10))\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)    \n",
    "\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "        \n",
    "        # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else \n",
    "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(batch_size):\n",
    "            label = target.data[i]\n",
    "            class_correct_train[label] += correct[i].item()\n",
    "            class_total_train[label] += 1        \n",
    "    \n",
    "#     print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "    \n",
    "    for data, target in valid_loader:\n",
    "        \n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)    \n",
    "\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "        \n",
    "        # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else \n",
    "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(batch_size):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1        \n",
    "        \n",
    "    \n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    list_train_losses.append(train_loss)\n",
    "    list_val_losses.append(valid_loss)\n",
    "\n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "\n",
    "    # BEST MODEL\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('SAVING NEW BEST MODEL')\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6896241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY BY CLASS\n",
      "\n",
      "b'airplane': 45%\n",
      "b'automobile': 61%\n",
      "b'bird': 32%\n",
      "b'cat': 32%\n",
      "b'deer': 40%\n",
      "b'dog': 38%\n",
      "b'frog': 58%\n",
      "b'horse': 53%\n",
      "b'ship': 61%\n",
      "b'truck': 59%\n",
      "\n",
      "Overall: 48% \n",
      "\n",
      "VALIDATION ACCURACY BY CLASS\n",
      "\n",
      "b'airplane': 59%\n",
      "b'automobile': 54%\n",
      "b'bird': 20%\n",
      "b'cat': 12%\n",
      "b'deer': 57%\n",
      "b'dog': 34%\n",
      "b'frog': 48%\n",
      "b'horse': 52%\n",
      "b'ship':  7%\n",
      "b'truck': 56%\n",
      "\n",
      "Overall: 40% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TRAINING ACCURACY\n",
    "print(\"TRAINING ACCURACY BY CLASS\\n\")\n",
    "for i in range(10):\n",
    "    if class_total_train[i] > 0:\n",
    "        print('%5s: %2d%%' % (\n",
    "            meta_data[b'label_names'][i], 100 * class_correct_train[i] / class_total_train[i]))\n",
    "    else:\n",
    "        print('Training Accuracy of %5s: N/A (no training examples)' % (meta_data[b'label_names'][i]))\n",
    "\n",
    "print('\\nOverall: %2d%%' % (\n",
    "    100. * np.sum(class_correct_train) / np.sum(class_total_train)),\"\\n\")     \n",
    "\n",
    "#VALIDATION ACCURACY\n",
    "print(\"VALIDATION ACCURACY BY CLASS\\n\")\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('%5s: %2d%%' % (\n",
    "            meta_data[b'label_names'][i], 100 * class_correct[i] / class_total[i]))\n",
    "    else:\n",
    "        print('Training Accuracy of %5s: N/A (no training examples)' % (meta_data[b'label_names'][i]))\n",
    "\n",
    "print('\\nOverall: %2d%%' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total)),\"\\n\")         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2033b253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_cifar.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8046ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.460338\n",
      "\n",
      "TESTING ACCURACY BY CLASS\n",
      "\n",
      "b'airplane': 58%\n",
      "b'automobile': 73%\n",
      "b'bird': 31%\n",
      "b'cat': 32%\n",
      "b'deer': 25%\n",
      "b'dog': 43%\n",
      "b'frog': 62%\n",
      "b'horse': 52%\n",
      "b'ship': 50%\n",
      "b'truck': 50%\n",
      "\n",
      "Overall: 48% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "for data, target in test_loader:\n",
    "    \n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "    \n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    \n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # update average validation loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    \n",
    "    # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else \n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    \n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "#         print('Loss: {:.4f}'.format(loss.item()))\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "#VALIDATION ACCURACY\n",
    "print(\"TESTING ACCURACY BY CLASS\\n\")\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('%5s: %2d%%' % (\n",
    "            meta_data[b'label_names'][i], 100 * class_correct[i] / class_total[i]))\n",
    "    else:\n",
    "        print('Training Accuracy of %5s: N/A (no training examples)' % (meta_data[b'label_names'][i]))\n",
    "\n",
    "print('\\nOverall: %2d%%' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total)),\"\\n\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
